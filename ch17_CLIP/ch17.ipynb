{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39503296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/c2/da/a5622266952ab05dc3995d77689cba600e49ea9d6c51d469c077695cb719/matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/aa/55/02c6d24804592b862b38a85c9b3283edc245081390a520ccd11697b6b24f/contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/2b/e8/61b8525acf26ec222518bdff127ae502bfa3408981fb5e5493f2b037d7fb/fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.5.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Using cached fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.0\n",
      "    Uninstalling pyparsing-3.1.0:\n",
      "      Successfully uninstalled pyparsing-3.1.0\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.1 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d2da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.14.0)\n",
      "Requirement already satisfied: tensorflow<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.52.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (3.6.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (13.0.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.22.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.14,>=2.13.0->tensorflow-text) (0.30.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.14,>=2.13.0->tensorflow-text) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow<2.14,>=2.13.0->tensorflow-text) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7548d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.22.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885001bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c29f1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a8a278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is downloaded and extracted successfully.\n",
      "Number of images: 82783\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"datasets\"\n",
    "annotations_dir = os.path.join(root_dir, \"annotations\")\n",
    "images_dir = os.path.join(root_dir, \"train2014\")\n",
    "tfrecords_dir = os.path.join(root_dir, \"tfrecords\")\n",
    "annotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n",
    "\n",
    "# Download caption annotation files\n",
    "if not os.path.exists(annotations_dir):\n",
    "    annotation_zip = tf.keras.utils.get_file(\n",
    "        \"captions.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(annotation_zip)\n",
    "\n",
    "# Download image files\n",
    "if not os.path.exists(images_dir):\n",
    "    image_zip = tf.keras.utils.get_file(\n",
    "        \"train2014.zip\",\n",
    "        cache_dir=os.path.abspath(\".\"),\n",
    "        origin=\"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "        extract=True,\n",
    "    )\n",
    "    os.remove(image_zip)\n",
    "\n",
    "print(\"Dataset is downloaded and extracted successfully.\")\n",
    "\n",
    "with open(annotation_file, \"r\") as f:\n",
    "    annotations = json.load(f)[\"annotations\"]\n",
    "\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "for element in annotations:\n",
    "    caption = f\"{element['caption'].lower().rstrip('.')}\"\n",
    "    image_path = images_dir + \"/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2f9529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:49<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 training examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 evaluation examples were written to tfrecord files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_size = 30000\n",
    "valid_size = 5000\n",
    "captions_per_image = 2\n",
    "images_per_file = 2000\n",
    "\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-valid_size:]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file))\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca7ed6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38014a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f62e11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06353313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        name = \"bert\",\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b1f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb84a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5  # In practice, train for at least 30 epochs\n",
    "batch_size = 256\n",
    "\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "867ea603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2ee772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 20:56:33.068104: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-23 20:56:33.111514: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-23 20:56:33.112344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.backend' has no attribute 'tensorflow_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[0;32m----> 2\u001b[0m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensorflow_backend\u001b[49m\u001b[38;5;241m.\u001b[39m_get_available_gpus()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.backend' has no attribute 'tensorflow_backend'"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 0\n",
      "Number of examples (caption-image pairs): 60000\n",
      "Batch size: 256\n",
      "Steps per epoch: 235\n",
      "Epoch 1/5\n",
      "      1/Unknown - 40s 40s/step - loss: 647.5156"
     ]
    }
   ],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef809f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading vision and text encoders...\")\n",
    "vision_encoder = keras.models.load_model(\"vision_encoder\")\n",
    "text_encoder = keras.models.load_model(\"text_encoder\")\n",
    "print(\"Models are loaded.\")\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return tf.image.resize(image_array, (299, 299))\n",
    "\n",
    "\n",
    "print(f\"Generating embeddings for {len(image_paths)} images...\")\n",
    "image_embeddings = vision_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_matches(image_embeddings, queries, k=9, normalize=True):\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder(tf.convert_to_tensor(queries))\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[image_paths[idx] for idx in indices] for indices in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a family standing next to the ocean on a sandy beach with a surf board\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(matches[i]))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73564237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_top_k_accuracy(image_paths, k=100):\n",
    "    hits = 0\n",
    "    num_batches = int(np.ceil(len(image_paths) / batch_size))\n",
    "    for idx in tqdm(range(num_batches)):\n",
    "        start_idx = idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        current_image_paths = image_paths[start_idx:end_idx]\n",
    "        queries = [\n",
    "            image_path_to_caption[image_path][0] for image_path in current_image_paths\n",
    "        ]\n",
    "        result = find_matches(image_embeddings, queries, k)\n",
    "        hits += sum(\n",
    "            [\n",
    "                image_path in matches\n",
    "                for (image_path, matches) in list(zip(current_image_paths, result))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return hits / len(image_paths)\n",
    "\n",
    "\n",
    "print(\"Scoring training data...\")\n",
    "train_accuracy = compute_top_k_accuracy(train_image_paths)\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n",
    "\n",
    "print(\"Scoring evaluation data...\")\n",
    "eval_accuracy = compute_top_k_accuracy(image_paths[train_size:])\n",
    "print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0922e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
